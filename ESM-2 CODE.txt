import torch
import pandas as pd
from transformers import EsmModel, EsmTokenizer

# Load ESM-2 model and tokenizer (650M variant)
model_name = "facebook/esm2_t33_650M_UR50D"
tokenizer = EsmTokenizer.from_pretrained(model_name)
model = EsmModel.from_pretrained(model_name)

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Function to generate ESM-2 embeddings
def get_esm2_embedding(sequence):
    inputs = tokenizer(sequence, return_tensors="pt", add_special_tokens=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        token_embeddings = outputs.last_hidden_state
        # Average embeddings, excluding special tokens ([CLS] and [EOS])
        embedding = token_embeddings[0, 1:-1].mean(dim=0).cpu().numpy()
    return embedding

# Read sequences from a text file (one epitope per line)
input_file = "negative_epitopes.txt"  # Change this to your text file name
with open(input_file, "r") as f:
    epitopes = [line.strip() for line in f if line.strip()]

# Process and collect embeddings
data = []
for epitope in epitopes:
    embedding = get_esm2_embedding(epitope)
    data.append([epitope] + embedding.tolist())

# Save to CSV
df = pd.DataFrame(data)
df.columns = ['Epitope'] + [f'Feature_{i}' for i in range(1, 1281)]
df.to_csv("non_epitope_embeddings.csv", index=False)
print("Embeddings saved to epitope_embeddings.csv")
